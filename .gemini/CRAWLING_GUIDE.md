# ğŸ¤– ìë™ í¬ë¡¤ë§ ì‹œìŠ¤í…œ êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ìš”êµ¬ì‚¬í•­

- **ìŠ¤ì¼€ì¤„**: ë§¤ì¼ ì˜¤ì „ 6ì‹œ ìë™ ì‹¤í–‰
- **í”„ë¡œì„¸ìŠ¤**: í¬ë¡¤ë§ â†’ ê´€ë¦¬ì ìŠ¹ì¸ â†’ ê²Œì‹œ
- **ëŒ€ìƒ í˜ì´ì§€**: `/recruit` (ì—°ê²° - ì±„ìš©Â·ê³µëª¨ì „Â·ì´ë²¤íŠ¸)

---

## âŒ Next.js í¬ë¡¤ë§ì˜ ë¬¸ì œì 

### 1. **ì„œë²„ë¦¬ìŠ¤ í™˜ê²½ì˜ ì œì•½**

- Vercel/Netlify ê°™ì€ í”Œë«í¼ì—ì„œëŠ” í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì œí•œ (10~60ì´ˆ)
- í¬ë¡¤ë§ì€ ì—¬ëŸ¬ ì‚¬ì´íŠ¸ë¥¼ ìˆœíšŒí•˜ë¯€ë¡œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼
- ë©”ëª¨ë¦¬ ì œí•œ (1GB ì´í•˜)

### 2. **ìŠ¤ì¼€ì¤„ë§ ì–´ë ¤ì›€**

- Next.js ìì²´ì—ëŠ” cron job ê¸°ëŠ¥ì´ ì—†ìŒ
- ë§¤ì¼ íŠ¹ì • ì‹œê°„ ì‹¤í–‰ì´ ë³µì¡í•¨

### 3. **ë¹„ìš© ë¬¸ì œ**

- ì„œë²„ë¦¬ìŠ¤ í•¨ìˆ˜ í˜¸ì¶œë§ˆë‹¤ ê³¼ê¸ˆ
- í¬ë¡¤ë§ì€ ë¦¬ì†ŒìŠ¤ë¥¼ ë§ì´ ì‚¬ìš©

---

## âœ… ì¶”ì²œ í•´ê²°ì±… (3ê°€ì§€ ì˜µì…˜)

### **Option 1: Vercel Cron Jobs** (ê°€ì¥ ê°„ë‹¨)

**ì¥ì :**

- Vercelì— ë‚´ì¥ëœ ê¸°ëŠ¥
- ì„¤ì •ì´ ë§¤ìš° ê°„ë‹¨
- ì¶”ê°€ ì„œë¹„ìŠ¤ ë¶ˆí•„ìš”

**ë‹¨ì :**

- Vercel Pro í”Œëœ í•„ìš” ($20/ì›”)
- ì‹¤í–‰ ì‹œê°„ ì œí•œ (60ì´ˆ)

**êµ¬í˜„ ë°©ë²•:**

```json
// vercel.json
{
  "crons": [
    {
      "path": "/api/cron/crawl-news",
      "schedule": "0 6 * * *" // ë§¤ì¼ ì˜¤ì „ 6ì‹œ (UTC ê¸°ì¤€ì´ë¯€ë¡œ 21:00 = KST 06:00)
    }
  ]
}
```

```typescript
// src/app/api/cron/crawl-news/route.ts
import { NextRequest, NextResponse } from "next/server";
import { supabaseAdmin } from "@/lib/supabase/admin";

export async function GET(request: NextRequest) {
  // Vercel Cron Secretìœ¼ë¡œ ë³´ì•ˆ í™•ì¸
  const authHeader = request.headers.get("authorization");
  if (authHeader !== `Bearer ${process.env.CRON_SECRET}`) {
    return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
  }

  try {
    // 1. í¬ë¡¤ë§ ì‹¤í–‰
    const crawledItems = await crawlRecruitSites();

    // 2. DBì— ì €ì¥ (is_approved = false)
    const { data, error } = await supabaseAdmin.from("recruit_items").insert(
      crawledItems.map((item) => ({
        ...item,
        is_approved: false, // ê´€ë¦¬ì ìŠ¹ì¸ ëŒ€ê¸°
        is_active: false, // ë¹„í™œì„± ìƒíƒœ
        crawled_at: new Date().toISOString(),
      }))
    );

    if (error) throw error;

    return NextResponse.json({
      success: true,
      count: crawledItems.length,
    });
  } catch (error) {
    console.error("Crawl error:", error);
    return NextResponse.json({ error: "Crawl failed" }, { status: 500 });
  }
}

// í¬ë¡¤ë§ í•¨ìˆ˜ (ì˜ˆì‹œ)
async function crawlRecruitSites() {
  const items = [];

  // ì˜ˆì‹œ: ê³µëª¨ì „ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
  const contestSites = [
    "https://www.wevity.com",
    "https://www.thinkcontest.com",
  ];

  for (const site of contestSites) {
    // Cheerioë‚˜ Puppeteer ì‚¬ìš©
    const siteItems = await crawlSite(site);
    items.push(...siteItems);
  }

  return items;
}
```

---

### **Option 2: GitHub Actions** (ë¬´ë£Œ, ì¶”ì²œ!)

**ì¥ì :**

- ì™„ì „ ë¬´ë£Œ
- ì•ˆì •ì ì¸ ìŠ¤ì¼€ì¤„ë§
- ì‹¤í–‰ ì‹œê°„ ì œí•œ ì—†ìŒ (6ì‹œê°„ê¹Œì§€)

**ë‹¨ì :**

- GitHub ì €ì¥ì†Œ í•„ìš”
- ì„¤ì •ì´ ì•½ê°„ ë³µì¡

**êµ¬í˜„ ë°©ë²•:**

```yaml
# .github/workflows/daily-crawl.yml
name: Daily News Crawl

on:
  schedule:
    - cron: "0 21 * * *" # UTC 21:00 = KST 06:00
  workflow_dispatch: # ìˆ˜ë™ ì‹¤í–‰ë„ ê°€ëŠ¥

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: "18"

      - name: Install dependencies
        run: npm install

      - name: Run crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: node scripts/crawl-recruit.js
```

```javascript
// scripts/crawl-recruit.js
const { createClient } = require("@supabase/supabase-js");
const cheerio = require("cheerio");

const supabase = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_SERVICE_KEY
);

async function main() {
  console.log("Starting crawl...");

  // í¬ë¡¤ë§ ë¡œì§
  const items = await crawlAllSites();

  // DBì— ì €ì¥
  const { data, error } = await supabase.from("recruit_items").insert(
    items.map((item) => ({
      ...item,
      is_approved: false,
      is_active: false,
      crawled_at: new Date().toISOString(),
    }))
  );

  if (error) {
    console.error("Error:", error);
    process.exit(1);
  }

  console.log(`Successfully crawled ${items.length} items`);
}

main();
```

---

### **Option 3: Supabase Edge Functions** (ê°€ì¥ ì•ˆì •ì )

**ì¥ì :**

- Supabase ìƒíƒœê³„ì™€ ì™„ë²½ í†µí•©
- ë¬´ë£Œ í‹°ì–´ ì œê³µ
- Deno ëŸ°íƒ€ì„ìœ¼ë¡œ ë¹ ë¦„

**ë‹¨ì :**

- Deno/TypeScript í•™ìŠµ í•„ìš”
- ë³„ë„ ë°°í¬ ê³¼ì • í•„ìš”

**êµ¬í˜„ ë°©ë²•:**

```bash
# Supabase CLI ì„¤ì¹˜
npm install -g supabase

# Edge Function ìƒì„±
supabase functions new crawl-recruit

# ë°°í¬
supabase functions deploy crawl-recruit
```

```typescript
// supabase/functions/crawl-recruit/index.ts
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";

serve(async (req) => {
  const supabase = createClient(
    Deno.env.get("SUPABASE_URL") ?? "",
    Deno.env.get("SUPABASE_SERVICE_ROLE_KEY") ?? ""
  );

  // í¬ë¡¤ë§ ë¡œì§
  const items = await crawlSites();

  // DB ì €ì¥
  const { data, error } = await supabase.from("recruit_items").insert(
    items.map((item) => ({
      ...item,
      is_approved: false,
      is_active: false,
    }))
  );

  return new Response(JSON.stringify({ success: true, count: items.length }), {
    headers: { "Content-Type": "application/json" },
  });
});
```

ê·¸ë¦¬ê³  **pg_cron**ìœ¼ë¡œ ìŠ¤ì¼€ì¤„ë§:

```sql
-- Supabase SQL Editorì—ì„œ ì‹¤í–‰
SELECT cron.schedule(
  'daily-crawl',
  '0 6 * * *',  -- ë§¤ì¼ ì˜¤ì „ 6ì‹œ
  $$
  SELECT net.http_post(
    url:='https://your-project.supabase.co/functions/v1/crawl-recruit',
    headers:='{"Authorization": "Bearer YOUR_ANON_KEY"}'::jsonb
  );
  $$
);
```

---

## ğŸ—„ï¸ DB ìŠ¤í‚¤ë§ˆ ìˆ˜ì •

ê´€ë¦¬ì ìŠ¹ì¸ ê¸°ëŠ¥ì„ ìœ„í•´ í…Œì´ë¸”ì— ì»¬ëŸ¼ ì¶”ê°€:

```sql
-- recruit_items í…Œì´ë¸”ì— ì»¬ëŸ¼ ì¶”ê°€
ALTER TABLE recruit_items
ADD COLUMN IF NOT EXISTS is_approved BOOLEAN DEFAULT false,
ADD COLUMN IF NOT EXISTS crawled_at TIMESTAMP,
ADD COLUMN IF NOT EXISTS approved_at TIMESTAMP,
ADD COLUMN IF NOT EXISTS approved_by UUID REFERENCES auth.users(id);

-- ì¸ë±ìŠ¤ ì¶”ê°€ (ì„±ëŠ¥ í–¥ìƒ)
CREATE INDEX IF NOT EXISTS idx_recruit_items_approved
ON recruit_items(is_approved, is_active);
```

---

## ğŸ‘¨â€ğŸ’¼ ê´€ë¦¬ì ìŠ¹ì¸ í˜ì´ì§€

`/admin/recruit-approval` í˜ì´ì§€ ìƒì„± í•„ìš”:

```typescript
// src/app/admin/recruit-approval/page.tsx
export default function RecruitApprovalPage() {
  const [pendingItems, setPendingItems] = useState([]);

  // is_approved = falseì¸ í•­ëª© ì¡°íšŒ
  useEffect(() => {
    loadPendingItems();
  }, []);

  const handleApprove = async (id: number) => {
    await supabase
      .from("recruit_items")
      .update({
        is_approved: true,
        is_active: true,
        approved_at: new Date().toISOString(),
      })
      .eq("id", id);

    loadPendingItems();
  };

  const handleReject = async (id: number) => {
    await supabase.from("recruit_items").delete().eq("id", id);

    loadPendingItems();
  };

  // UI ë Œë”ë§...
}
```

---

## ğŸ¯ ìµœì¢… ì¶”ì²œ

**GitHub Actions (Option 2)** ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤!

**ì´ìœ :**

1. âœ… ì™„ì „ ë¬´ë£Œ
2. âœ… ì•ˆì •ì ì´ê³  ì‹ ë¢°ì„± ë†’ìŒ
3. âœ… ì‹¤í–‰ ì‹œê°„ ì œí•œ ì—†ìŒ
4. âœ… ì„¤ì •ì´ ë¹„êµì  ê°„ë‹¨
5. âœ… ì´ë¯¸ GitHub ì‚¬ìš© ì¤‘

**ë‹¤ìŒ ë‹¨ê³„:**

1. `scripts/crawl-recruit.js` í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
2. `.github/workflows/daily-crawl.yml` ì›Œí¬í”Œë¡œìš° ì„¤ì •
3. DB ìŠ¤í‚¤ë§ˆ ìˆ˜ì • (is_approved ì»¬ëŸ¼ ì¶”ê°€)
4. ê´€ë¦¬ì ìŠ¹ì¸ í˜ì´ì§€ êµ¬í˜„

---

## ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€

```bash
npm install cheerio axios
# ë˜ëŠ” ë” ê°•ë ¥í•œ í¬ë¡¤ë§ì„ ìœ„í•´
npm install puppeteer
```

---

## ğŸ” ë³´ì•ˆ ì£¼ì˜ì‚¬í•­

1. **Secrets ê´€ë¦¬**: GitHub Secretsì— API í‚¤ ì €ì¥
2. **Rate Limiting**: í¬ë¡¤ë§ ì‹œ ìš”ì²­ ê°„ê²© ë‘ê¸°
3. **User-Agent**: ë´‡ìœ¼ë¡œ ì°¨ë‹¨ë˜ì§€ ì•Šë„ë¡ ì„¤ì •
4. **Robots.txt**: í¬ë¡¤ë§ í—ˆìš© ì—¬ë¶€ í™•ì¸

---

êµ¬í˜„ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”! ğŸš€
